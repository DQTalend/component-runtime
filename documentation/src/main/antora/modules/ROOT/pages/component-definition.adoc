= Components Definition
:page-partial:

Talend Component Kit framework relies on several primitive components.

All components can use `@PostConstruct` and `@PreDestroy` annotations to initialize or release some underlying resource at the beginning and the end of a processing.

IMPORTANT: In distributed environments, class constructor are called on cluster manager node. Methods annotated with `@PostConstruct` and `@PreDestroy` are called on worker nodes. Thus, partition plan computation and pipeline tasks are performed on different nodes.

////
[ditaa, generated-deployment-diagram, png]
....
                 /-------------------------\
                 |    Create and submit    |
                 |   task to cluster (1)   |
                 \-------------------------/
                             |
                             V
                +---------------------------+
                |     Cluster manager       |
                |---------------------------|
                |     Partition plan        |
                |     computation (2)       |
                |                           |
                +---------------------------+
                             ^
                             |
                          Serialized
                          instances
                             |
                             V
                    +------------------+
                    |   Worker node    |
                    |----------------- |
                    |Flow Execution (3)|
                    +------------------+
....
////
image:deployment-diagram.png[]

1. The created task is a JAR file containing class information, which describes the pipeline (flow) that should be processed in cluster.
2. During the partition plan computation step, the pipeline is analyzed and split into stages. The cluster manager node instantiates mappers/processors, gets estimated data size using mappers, and splits created mappers according to the estimated data size. +
All instances are then serialized and sent to the worker node.
3. Serialized instances are received and deserialized. Methods annotated with `@PostConstruct` are called. After that, pipeline execution starts. The @BeforeGroup annotated method of the processor is called before processing the first element in chunk. +
After processing the number of records estimated as chunk size, the `@AfterGroup` annotated method of the processor is called. Chunk size is calculated depending on the environment the pipeline is processed by. Once the pipeline is processed, methods annotated with `@PreDestroy` are called.

IMPORTANT: All the methods managed by the framework must be public. Private methods are ignored.

////
[ditaa, generated-driver-processing-workflow, png]
....
Partition plan computation (2)
    +----------------+
    | Create Mappers |
    +----------------+
            |
            V
+--------------------------+
|Compute partition plan (2)|
+--------------------------+
            |
            V
  +----------------------+
  |  Serialize split     |
  |mappers and processors|
  +----------------------+
....
////
image:driver-processing-workflow.png[]

////
[ditaa, generated-worker-processing-workflow, png]
....
Flow Execution (3)
+------------------+
|  @PostConstruct  |
|     methods      |
+------------------+
         |
         V
+------------------+
|  @BeforeGroup    |
|     methods      |
+------------------+
         |
         V
+------------------+
|   Performs task  |
|   described in   |
|     pipeline     |
+------------------+
         |
         V
+------------------+
|   @AfterGroup    |
|     methods      |
+------------------+
         |
         V
+------------------+
|   @PreDestroy    |
|     methods      |
+------------------+
....
////
image:worker-processing-workflow.png[]

NOTE: The framework is designed to be as declarative as possible but also to stay extensible by not using fixed interfaces or method signatures. This allows to incrementally add new features of the underlying implementations.

== PartitionMapper

A `PartitionMapper` is a component able to split itself to make the execution more efficient.

This concept is borrowed from big data and useful in this context only (`BEAM` executions).
The idea is to divide the work before executing it in order to reduce the overall execution time.

The process is the following:

1. The size of the data you work on is estimated. This part can be heuristic and not very precise.
2. From that size, the execution engine (_runner_ for beam) requests the mapper to split _itself_ in _N_ mappers with a subset of the overall work.
3. The _leaf_ (final) mappers is used as a `Producer` (actual reader) factory.

IMPORTANT: This kind of component must be `Serializable` to be distributable.

=== Definition

A partition mapper requires three methods marked with specific annotations:

1. `@Assessor` for the evaluating method
2. `@Split` for the dividing method
3. `@Emitter` for the `Producer` factory

==== @Assessor

The Assessor method returns the estimated size of the data related to the component (depending its configuration).
It must return a `Number` and must not take any parameter.

For example:

[source,java,indent=0,subs="verbatim,quotes,attributes",role="initial-block-closed"]
----
@Assessor
public long estimateDataSetByteSize() {
    return ....;
}
----

==== @Split

The Split method returns a collection of partition mappers and can take optionally a `@PartitionSize` long value as parameter, which is the requested size of the dataset per sub partition mapper.

For example:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Split
public List<MyMapper> split(@PartitionSize final long desiredSize) {
    return ....;
}
----

==== @Emitter

The Emitter method must not have any parameter and must return a producer. It uses the partition mapper configuration to instantiate and configure the producer.

For example:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Emitter
public MyProducer create() {
    return ....;
}
----

== Producer

AÂ `Producer` is a component interacting with a physical source. It produces input data for the processing flow.

A producer is a simple component that must have a `@Producer` method without any parameter. It can return any data:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Producer
public MyData produces() {
    return ...;
}
----

== Processor

A `Processor` is a component converts incoming data to a different model.

A processor must have a method decorated with `@ElementListener` taking an incoming data and returning the processed data:

[source,java]
----
@ElementListener
public MyNewData map(final MyData data) {
    return ...;
}
----

IMPORTANT: this kind of component `MUST` be `Serializable` since it is distributed.

IMPORTANT: if you don't care much of the type of the parameter and need to access data on a "map like" based rule set, then you can
use `JsonObject` as parameter type and Talend Component will just wrap the data to enable you to access it as a map. The parameter
type is not enforced, i.e. if you know you will get a `SuperCustomDto` then you can use that as parameter type but for generic
component reusable in any chain it is more than highly encouraged to use `JsonObject` until you have your an evaluation language
based processor (which has its own way to access component). Here is an example:

[source,java]
----
@ElementListener
public MyNewData map(final JsonObject incomingData) {
    String name = incomingData.getString("name");
    int name = incomingData.getInt("age");
    return ...;
}

// equivalent to (using POJO subclassing)

public class Person {
    private String age;
    private int age;

    // getters/setters
}

@ElementListener
public MyNewData map(final Person person) {
    String name = person.getName();
    int age = person.getAge();
    return ...;
}

----

A processor also supports `@BeforeGroup` and `@AfterGroup` which `MUST` be methods without parameters and returning `void` (result would be ignored).
This is used by the runtime to mark a chunk of the data in a way which is estimated _good_ for the execution flow size.

IMPORTANT: this is estimated so you don't have any guarantee on the size of a _group_. You can literally have groups of size 1.

The common usage is to batch records for performance reasons:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@BeforeGroup
public void initBatch() {
    // ...
}

@AfterGroup
public void endBatch() {
    // ...
}
----

IMPORTANT: it is a good practise to support a `maxBatchSize` here and potentially commit before the end of the group in case
of a computed size which is way too big for your backend.

== Multiple outputs

In some case you may want to split the output of a processor in two. A common example is "main" and "reject" branches
where part of the incoming data are put in a specific bucket to be processed later.

This can be done using `@Output`. This can be used as a replacement of the returned value:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@ElementListener
public void map(final MyData data, @Output final OutputEmitter<MyNewData> output) {
    output.emit(createNewData(data));
}
----

Or you can pass it a string which will represent the new branch:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@ElementListener
public void map(final MyData data,
                @Output final OutputEmitter<MyNewData> main,
                @Output("rejected") final OutputEmitter<MyNewDataWithError> rejected) {
    if (isRejected(data)) {
        rejected.emit(createNewData(data));
    } else {
        main.emit(createNewData(data));
    }
}

// or simply

@ElementListener
public MyNewData map(final MyData data,
                    @Output("rejected") final OutputEmitter<MyNewDataWithError> rejected) {
    if (isSuspicious(data)) {
        rejected.emit(createNewData(data));
        return createNewData(data); // in this case we continue the processing anyway but notified another channel
    }
    return createNewData(data);
}
----

== Multiple inputs

Having multiple inputs is closeto the output case excep it doesn't require a wrapper `OutputEmitter`:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@ElementListener
public MyNewData map(@Input final MyData data, @Input("input2") final MyData2 data2) {
    return createNewData(data1, data2);
}
----

`@Input` takes the input name as parameter, if not set it uses the main (default) input branch.

IMPORTANT: due to the work required to not use the default branch it is recommended to use it when possible and not
name its branches depending on the component semantic.

== Output

____
An `Output` is a `Processor` returning no data.
____

Conceptually an output is a listener of data. It perfectly matches the concept of processor. Being the last of the execution chain
or returning no data will make your processor an output:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@ElementListener
public void store(final MyData data) {
    // ...
}
----

== Combiners?

For now Talend Component doesn't enable you to define a `Combiner`. It would be the symmetric part of the partition mapper
and allow to aggregate results in a single one.

== Family & Components Icons
Every component family and component need to have a representative icon.
You can use one of the icons provided by the component framework or you can use a custom icon.
For the component family the icon is defined in `package-info.java` and for the component it need to be declared in the component class.

To use a custom icon, you need to have the icon file placed in `resources/icons` folder of the project.
The icon file need to have a name following the convention `IconName_icon32.png`

[source,java]
----
@Icon(value = Icon.IconType.CUSTOM, custom = "IconName")
----
