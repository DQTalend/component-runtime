= Talend Component Documentation
:toc:
:numbered:
:icons: font
:hide-uri-scheme:
:imagesdir: images
:outdir: ../assets
:jbake-type: page
:jbake-tags: documentation
:jbake-status: published

== Components Definitions

Talend Component framework relies on several primitive components.

They can all use `@PostConstruct` and `@PreDestroy` to initialize/release
some underlying resource at the beginning/end of the processing.

IMPORTANT: in distributed environments class' constructor will be called on cluster manager node, methods annotated with
`@PostConstruct` and `@PreDestroy` annotations will be called on worker nodes. Thus, partition plan computation and pipeline task
will be performed on different nodes.

[ditaa, generated-deployment-diagram, png]
....
                 /-------------------------\
                 |       Create and        |
                 |Submit task to cluster(1)|
                 \-------------------------/
                             |
                             V
                +---------------------------+
                |     Cluster manager       |
                |---------------------------|
                |     Partition plan        |
                |     computation(2)        |
                |                           |
                +---------------------------+
                             ^
                             |
                          Serialized
                          instances
                             |
                             V
                    +-----------------+
                    |   Worker node   |
                    |-----------------|
                    |Flow Execution(3)|
                    +-----------------+
....

1. Created task consists of Jar file, containing class, which describes pipeline(flow) which should be processed in cluster.
2. During partition plan computation step pipeline is analyzed and split into stages. Cluster Manager node instantiates mappers/processors
gets estimated data size using mappers, splits created mappers according to the estimated data size. All instances are serialized and
sent to Worker nodes afterwards.
3. Serialized instances are received and deserialized, methods annotated with @PostConstruct annotation are called. After that,
pipeline execution is started. Processor's @BeforeGroup annotated method is called before processing first element in chunk.
After processing number of records estimated as chunk size, Processor's @AfterGroup annotated method called. Chunk size is calculated
depending on environment the pipeline is processed by. After pipeline is processed, methods annotated with @PreDestroy annotation are called.

[ditaa, generated-driver-processing-workflow, png]
....
Partition plan computation(2)
    +----------------+
    | Create Mappers |
    +----------------+
            |
            V
+-------------------------+
|Compute partition plan(2)|
+-------------------------+
            |
            V
  +----------------------+
  |  Serialize splitted  |
  |mappers and processors|
  +----------------------+
....

[ditaa, generated-worker-processing-workflow, png]
....
Flow Execution(3)
+------------------+
|  @PostConstruct  |
|     methods      |
+------------------+
         |
         V
+------------------+
|  @BeforeGroup    |
|     methods      |
+------------------+
         |
         V
+------------------+
|   Perform task   |
|   described in   |
|     pipeline     |
+------------------+
         |
         V
+------------------+
|   @AfterGroup    |
|     methods      |
+------------------+
         |
         V
+------------------+
|   @PreDestroy    |
|     methods      |
+------------------+
....

IMPORTANT: all framework managed methods `MUST` be public too. Private methods are ignored.

NOTE: in term of design the framework tries to be as declarative as possible but also to stay extensible
not using fixed interfaces or method signatures. This will allow to add incrementally new features of the underlying implementations.

=== PartitionMapper

____
A `PartitionMapper` is a component able to split itself to
make the execution more efficient.
____

This concept is borrowed to big data world and useful only in this context (`BEAM` executions).
Overall idea is to divide the work before executing it to try to reduce the overall execution time.

The process is the following:

1. Estimate the size of the data you will work on. This part is often heuristic and not very precise.
2. From that size the execution engine (_runner_ for beam) will request the mapper to split _itself_ in _N_ mappers with a subset of the overall work.
3. The _leaf_ (final) mappers will be used as a `Producer` (actual reader) factory.

IMPORTANT: this kind of component `MUST` be `Serializable` to be distributable.

==== Definition

A partition mapper requires 3 methods marked with specific annotations:

1. `@Assessor` for the evaluating method
2. `@Split` for the dividing method
3. `@Emitter` for the `Producer` factory

===== @Assessor

The assessor method will return the estimated size of the data related to the component (depending its configuration).
It `MUST` return a `Number` and `MUST` not take any parameter.

Here is an example:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Assessor
public long estimateDataSetByteSize() {
    return ....;
}
----

===== @Split

The split method will return a collection of partition mappers and can take optionally a `@PartitionSize` long
value which is the requested size of the dataset per sub partition mapper.

Here is an example:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Split
public List<MyMapper> split(@PartitionSize final long desiredSize) {
    return ....;
}
----

===== @Emitter

The emitter method `MUST` not have any parameter and `MUST` return a producer. It generally uses the partition mapper configuration
to instantiate/configure the producer.

Here is an example:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Emitter
public MyProducer create() {
    return ....;
}
----

=== Producer

____
AÂ `Producer` is the component interacting with a physical source. It produces input data for the processing flow.
____

A producer is a very simple component which `MUST` have a `@Producer` method without any parameter and returning any data:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Producer
public MyData produces() {
    return ...;
}
----

=== Processor

____
A `Processor` is a component responsible to convert an incoming data to another model.
____

A processor `MUST` have a method decorated with `@ElementListener` taking an incoming data and returning the processed data:

[source,java]
----
@ElementListener
public MyNewData map(final MyData data) {
    return ...;
}
----

IMPORTANT: this kind of component `MUST` be `Serializable` since it is distributed.

IMPORTANT: if you don't care much of the type of the parameter and need to access data on a "map like" based rule set, then you can
use `JsonObject` as parameter type and Talend Component will just wrap the data to enable you to access it as a map. The parameter
type is not enforced, i.e. if you know you will get a `SuperCustomDto` then you can use that as parameter type but for generic
component reusable in any chain it is more than highly encouraged to use `JsonObject` until you have your an evaluation language
based processor (which has its own way to access component). Here is an example:

[source,java]
----
@ElementListener
public MyNewData map(final JsonObject incomingData) {
    String name = incomingData.getString("name");
    int name = incomingData.getInt("age");
    return ...;
}

// equivalent to (using POJO subclassing)

public class Person {
    private String age;
    private int age;

    // getters/setters
}

@ElementListener
public MyNewData map(final Person person) {
    String name = person.getName();
    int name = person.getAge();
    return ...;
}

----

A processor also supports `@BeforeGroup` and `@AfterGroup` which `MUST` be methods without parameters and returning `void` (result would be ignored).
This is used by the runtime to mark a chunk of the data in a way which is estimated _good_ for the execution flow size.

IMPORTANT: this is estimated so you don't have any guarantee on the size of a _group_. You can literally have groups of size 1.

The common usage is to batch records for performance reasons:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@BeforeGroup
public void initBatch() {
    // ...
}

@AfterGroup
public void endBatch() {
    // ...
}
----

IMPORTANT: it is a good practise to support a `maxBatchSize` here and potentially commit before the end of the group in case
of a computed size which is way too big for your backend.

=== Multiple outputs

In some case you may want to split the output of a processor in two. A common example is "main" and "reject" branches
where part of the incoming data are put in a specific bucket to be processed later.

This can be done using `@Output`. This can be used as a replacement of the returned value:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@ElementListener
public void map(final MyData data, @Output final OutputEmitter<MyNewData> output) {
    output.emit(createNewData(data));
}
----

Or you can pass it a string which will represent the new branch:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@ElementListener
public void map(final MyData data,
                @Output final OutputEmitter<MyNewData> main,
                @Output("rejected") final OutputEmitter<MyNewDataWithError> rejected) {
    if (isRejected(data)) {
        rejected.emit(createNewData(data));
    } else {
        main.emit(createNewData(data));
    }
}

// or simply

@ElementListener
public MyNewData map(final MyData data,
                    @Output("rejected") final OutputEmitter<MyNewDataWithError> rejected) {
    if (isSuspicious(data)) {
        rejected.emit(createNewData(data));
        return createNewData(data); // in this case we continue the processing anyway but notified another channel
    }
    return createNewData(data);
}
----

=== Multiple inputs

Having multiple inputs is closeto the output case excep it doesn't require a wrapper `OutputEmitter`:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@ElementListener
public MyNewData map(@Input final MyData data, @Input("input2") final MyData2 data2) {
    return createNewData(data1, data2);
}
----

`@Input` takes the input name as parameter, if not set it uses the main (default) input branch.

IMPORTANT: due to the work required to not use the default branch it is recommanded to use it when possible and not
name its branches depending on the component semantic.

==== Output

____
An `Output` is a `Processor` returning no data.
____

Conceptually an output is a listener of data. It perfectly matches the concept of processor. Being the last of the execution chain
or returning no data will make your processor an output:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@ElementListener
public void store(final MyData data) {
    // ...
}
----

=== Combiners?

For now Talend Component doesn't enable you to define a `Combiner`. It would be the symmetric part of the partition mapper
and allow to aggregate results in a single one.

== Registering components

As seen in the <<getting-started.adoc#getting-started-first-quick-start, Getting Started>>, you need an annotation to register
your component through `family` method. Multiple components can use the same `family` value but the pair `family`+`name`
`MUST` be unique for the system.

If you desire (recommended) to share the same component family name instead of repeating yourself in all `family` methods,
you can use `@Components` annotation on the root package of you component, it will enable you to define the component family and
the categories the component belongs to (default is `Misc` if not set). Here is a sample `package-info.java`:

[source,java]
----
@Components(name = "my_component_family", categories = "My Category")
package org.talend.sdk.component.sample;

import org.talend.sdk.component.api.component.Components;
----

For an existing component it can look like:

[source,java]
----
@Components(name = "Salesforce", categories = {"Business", "Cloud"})
package org.talend.sdk.component.sample;

import org.talend.sdk.component.api.component.Components;
----

=== Components metadata

Components can require a few metadata to be integrated in Talend Studio or Cloud platform. Here is how to provide these information.
These metadata are set on the component class and belongs to `org.talend.sdk.component.api.component` package.

[options="header,autowidth"]
|====
| API | Description
| @Icon | Set an icon key used to represent the component. Note you can use a custom key with `custom()` method but it is not guaranteed the icon will be rendered properly.
| @Version | Set the component version, default to 1.
|====

Example:

[source,java]
----
@Icon(FILE_XML_O)
@PartitionMapper(name = "jaxbInput")
public class JaxbPartitionMapper implements Serializable {
    // ...
}
----

==== Management of configuration versions

If some impacting changes happen on the configuration they can be manage through a migration handler at *component* level (to enable
to support trans-model migration).

The `@Version` annotation supports a `migrationHandler` method which will take the implementation migrating the incoming configuration
to the current model.

For instance if `filepath` configuration entry from v1 changed to `location` in v2 you can remap the value to the right key in your
`MigrationHandler` implementation.

TIP: it is recommanded to not manage all migrations in the handler but rather split it in services you inject in the migration handler
(through constructor):

[source,java]
----
// full component code structure skipped for brievity, kept only migration part
@Version(value = 3, migrationHandler = MyComponent.Migrations.class)
public class MyComponent {
    // the component code...


    private interface VersionConfigurationHandler {
        Map<String, String> migrate(Map<String, String> incomingData);
    }

    public static class Migrations {
        private final List<VersionConfigurationHandler> handlers;

        // VersionConfigurationHandler implementations are decorated with @Service
        public Migrations(final List<VersionConfigurationHandler> migrations) {
            this.handlers = migrations;
            this.handlers.sort(/*some custom logic*/);
        }

        @Override
        public Map<String, String> migrate(int incomingVersion, Map<String, String> incomingData) {
            Map<String, String> out = incomingData;
            for (MigrationHandler handler : handlers) {
                out = handler.migrate(out);
            }
        }
    }
}
----

What is important in this snippet is not much the way the code is organized but rather the fact you organize your migrations the way which fits the best
your component. If migrations are not conflicting no need of something fancy, just apply them all but if you need to apply them in order
you need to ensure they are sorted. Said otherwise: don't see this API as a migration API but as a migration callback
and adjust the migration code structure you need behind the `MigrationHandler` based on your
component requirements. The service injection enables you to do so.


==== @PartitionMapper

`@PartitionMapper` will obviously mark a partition mapper:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@PartitionMapper(family = "demo", name = "my_mapper")
public class MyMapper {
}
----

===== @Emitter

`@Emitter` is a shortcut for `@PartitionMapper` when you don't support distribution. Said otherwise it will enforce an implicit
partition mapper execution with an assessor size of 1 and a split returning itself.

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Emitter(family = "demo", name = "my_input")
public class MyInput {
}
----

==== @Processor

A method decorated with `@Processor` will be considered as a producer factory:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
@Processor(family = "demo", name = "my_processor")
public class MyProcessor {
}
----

== Internationalization

Recommanded practise for internationalization are:

* store messages using `ResourceBundle` properties file in your component module
* the location of the properties are in the same package than the related component(s) and is named `Messages` (ex: `org.talend.demo.MyComponent` will use `org.talend.demo.Messages[locale].properties`)
* for your own messages use the internationalization API

=== Internationalization API

Overal idea is to design its messages as methods returning `String` values
and back the template by a `ResourceBundle` located in the same package than the interface
defining these methods and named `Messages`.

IMPORTANT: this is the mecanism to use to internationalize your own messages in your own components.

To ensure you internationalization API is identified you need to mark it with `@Internationalized`:

[source,java]
----
@Internationalized <1>
public interface Translator {

    String message();

    String templatizedMessage(String arg0, int arg1); <2>

    String localized(String arg0, @Language Locale locale); <3>
}
----

<1> `@Internationalized` allows to mark a class as a i18n service
<2> you can pass parameters and the message will use `MessageFormat` syntax to be resolved based on the `ResourceBundle` template
<3> you can use `@Language` on a `Locale` parameter to specify manually the locale to use, note that a single value will be used (the first parameter tagged as such).

=== Default components keys

Out of the box components are internationalized using the same location logic for the resource bundle and here is the list
of supported keys:

[options="header,autowidth"]
|====
|Name Pattern|Description
|${family}._displayName|the display name of the family
|${family}.${configurationType}.${name}._displayName|the display name of a configuration type (dataStore or dataSet)
|${family}.${component_name}._displayName|the display name of the component (used by the GUIs)
|${property_path}._displayName|the display name of the option.
|${simple_class_name}.${property_name}._displayName|the display name of the option using it class name.
|${property_path}._placeholder|the placeholder of the option.
|====

Example of configuration for a component named `list` belonging to the family `memory` (`@Emitter(family = "memory", name = "list")`):

[source]
----
memory.list._displayName = Memory List
----

Configuration class are also translatable using the simple class name in the messages properties file.
This useful when you have some common configuration shared within multiple components.

If you have a configuration class like :
[source,java]
----
public class MyConfig {

    @Option
    private String host;

    @Option
    private int port;
}
----

You can give it a translatable display name by adding ${simple_class_name}.${property_name}._displayName to Messages.properties under the same package as the config class.
[source]
----
MyConfig.host._displayName = Server Host Name
MyConfig.host._placeholder = Enter Server Host Name...

MyConfig.port._displayName = Server Port
MyConfig.port._placeholder = Enter Server Port...
----

IMPORTANT: If you have a display name using the property path, it will override the display name defined using the simple class name.
this rule apply also to placeholders

== Component loading

Talend Component scanning is based on a plugin concept. To ensure plugins can be developped in parallel and avoid conflicts
it requires to isolate plugins (components or component grouped in a single jar/plugin).

Here we have multiple options which are (high level):

- flat classpath: listed for completeness but rejected _by design_ because it doesn't match at all this requirement.
- tree classloading: a shared classloader inherited by plugin classloaders but plugin classloader classes
are not seen by the shared classloader nor by other plugins.
- graph classloading: this one allows you to link the plugins and dependencies together dynamically in any direction.

If you want to map it to concrete common examples, the tree classloading is commonly used by Servlet containers where plugins are web applications
and the graph classloading can be illustrated by OSGi containers.

In the spirit of avoiding a lot of complexity added by this layer, Talend Component relies on a tree classloading. The advantage
is you don't need to define the relationship with other plugins/dependencies (it is built-in).

Here is a representation of this solution:

[ditaa, generated-classloader-layout, png]
....
                 /--------\
     +---------->| Shared |<---------+
     |           \--------/          |
     |               ^               |
     |               |               |
/----+-----\    /----+-----\    /----+-----\
| Plugin 1 |    | Plugin 2 |    | Plugin N |
\----------/    \----------/    \----------/
....

The interesting part is the _shared_ area will contain Talend Component API which is the only (by default) shared classes accross the whole plugins.

Then each plugins will be loaded in their own classloader with their dependencies.


=== Packaging a plugin

NOTE: this part explains the overall way to handle dependecnies but the Talend Maven plugin provides a shortcut for that.

A plugin is just a jar which was enriched with the list of its dependencies. By default Talend Component runtime is able to
read the output of `maven-dependency-plugin` in `TALEND-INF/dependencies.txt` location so you just need to ensure your component defines the following plugin:

[source,xml]
----
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-dependency-plugin</artifactId>
  <version>3.0.2</version>
  <executions>
    <execution>
      <id>create-TALEND-INF/dependencies.txt</id>
      <phase>process-resources</phase>
      <goals>
        <goal>list</goal>
      </goals>
      <configuration>
        <outputFile>${project.build.outputDirectory}/TALEND-INF/dependencies.txt</outputFile>
      </configuration>
    </execution>
  </executions>
</plugin>
----

If you check your jar once built you will see that the file contains something like:

[source,bash]
----
$ unzip -p target/mycomponent-1.0.0-SNAPSHOT.jar TALEND-INF/dependencies.txt

The following files have been resolved:
   org.talend.sdk.component:component-api:jar:1.0.0-SNAPSHOT:provided
   org.apache.geronimo.specs:geronimo-annotation_1.3_spec:jar:1.0:provided
   org.superbiz:awesome-project:jar:1.2.3:compile
   junit:junit:jar:4.12:test
   org.hamcrest:hamcrest-core:jar:1.3:test

----

What is important to see is the scope associated to the artifacts:

- the API (`component-api` and `geronimo-annotation_1.3_spec`) are `provided` because you can consider them to be there when executing (it comes with the framework)
- your specific dependencies (`awesome-project`) is `compile`: it will be included as a needed dependency by the framework (note that using `runtime` works too).
- the other dependencies will be ignored (`test` dependencies)

=== Packaging an application

Even if a flat classpath deployment is possible, it is not recommanded because it would then reduce the capabilities of the components.

==== Dependencies

The way the framework resolves dependencies is based on a local maven repository layout. As a quick reminder it looks like:

[source]
----
.
âââ groupId1
âÂ Â  âââ artifactId1
âÂ Â      âââ version1
âÂ Â      âÂ Â  âââ artifactId1-version1.jar
âÂ Â      âââ version2
âÂ Â       Â Â  âââ artifactId1-version2.jar
âââ groupId2
 Â Â  âââ artifactId2
 Â Â      âââ version1
 Â Â       Â Â  âââ artifactId2-version1.jar
----

This is all the layout the framework will use. Concretely the logic will convert the t-uple {groupId, artifactId, version, type (jar)}
to the path in the repository.

Talend Component runtime has two ways to find an artifact:

- from the file system based on a configure maven 2 repository.
- from a fatjar (uber jar) with a nested maven repository under `MAVEN-INF/repository`.

The first option will use either - by default - `${user.home}/.m2/repository` or a specific path configured when creating a `ComponentManager`.
The nested repository option will need some configuration during the packaging to ensure the repository is well created.

===== Create a nested maven repository with maven-shade-plugin

To create the nested `MAVEN-INF/repository` repository you can use `nested-maven-repository` extension:

[source,xml,indent=0,subs="verbatim,quotes,attributes"]
----
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-shade-plugin</artifactId>
  <version>3.0.0</version>
  <executions>
    <execution>
      <phase>package</phase>
      <goals>
        <goal>shade</goal>
      </goals>
      <configuration>
        <transformers>
          <transformer implementation="org.talend.sdk.component.container.maven.shade.ContainerDependenciesTransformer">
            <session>${session}</project>
          </transformer>
        </transformers>
      </configuration>
    </execution>
  </executions>
  <dependencies>
    <dependency>
      <groupId>org.talend.sdk.component</groupId>
      <artifactId>nested-maven-repository</artifactId>
      <version>${the.plugin.version}</version>
    </dependency>
  </dependencies>
</plugin>
----

==== Listing needed plugins

Plugin are programmatically registered in general but if you want to make some of them automatically available you
need to generate a `TALEND-INF/plugins.properties` which will map a plugin name to coordinates found with the maven mecanism
we just talked about.

Here again we can enrich `maven-shade-plugin` to do it:

[source,xml,indent=0,subs="verbatim,quotes,attributes"]
----
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-shade-plugin</artifactId>
  <version>3.0.0</version>
  <executions>
    <execution>
      <phase>package</phase>
      <goals>
        <goal>shade</goal>
      </goals>
      <configuration>
        <transformers>
          <transformer implementation="org.talend.sdk.component.container.maven.shade.PluginTransformer">
            <session>${session}</project>
          </transformer>
        </transformers>
      </configuration>
    </execution>
  </executions>
  <dependencies>
    <dependency>
      <groupId>org.talend.sdk.component</groupId>
      <artifactId>nested-maven-repository</artifactId>
      <version>${the.plugin.version}</version>
    </dependency>
  </dependencies>
</plugin>
----

==== `maven-shade-plugin` extensions

Here is a final job/application bundle based on maven shade plugin:

[source,xml,indent=0,subs="verbatim,quotes,attributes"]
----
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-shade-plugin</artifactId>
  <version>3.0.0</version>
  <configuration>
    <createDependencyReducedPom>false</createDependencyReducedPom>
    <filters>
      <filter>
        <artifact>*:*</artifact>
        <excludes>
          <exclude>META-INF/*.SF</exclude>
          <exclude>META-INF/*.DSA</exclude>
          <exclude>META-INF/*.RSA</exclude>
        </excludes>
      </filter>
    </filters>
  </configuration>
  <executions>
    <execution>
      <phase>package</phase>
      <goals>
        <goal>shade</goal>
      </goals>
      <configuration>
        <shadedClassifierName>shaded</shadedClassifierName>
        <transformers>
          <transformer
              implementation="org.talend.sdk.component.container.maven.shade.ContainerDependenciesTransformer">
            <session>${session}</session>
            <userArtifacts>
              <artifact>
                <groupId>org.talend.sdk.component</groupId>
                <artifactId>sample-component</artifactId>
                <version>1.0</version>
                <type>jar</type>
              </artifact>
            </userArtifacts>
          </transformer>
          <transformer implementation="org.talend.sdk.component.container.maven.shade.PluginTransformer">
            <session>${session}</session>
            <userArtifacts>
              <artifact>
                <groupId>org.talend.sdk.component</groupId>
                <artifactId>sample-component</artifactId>
                <version>1.0</version>
                <type>jar</type>
              </artifact>
            </userArtifacts>
          </transformer>
        </transformers>
      </configuration>
    </execution>
  </executions>
  <dependencies>
    <dependency>
      <groupId>org.talend.sdk.component</groupId>
      <artifactId>nested-maven-repository-maven-plugin</artifactId>
      <version>${the.version}</version>
    </dependency>
  </dependencies>
</plugin>
----

NOTE: the configuration unrelated to transformers can depend your application.

`ContainerDependenciesTransformer` is the one to embed a maven repository and `PluginTransformer` to create a file listing (one per line)
a list of artifacts (representing plugins).

Both transformers share most of their configuration:

- `session`: must be set to `${session}`. This is used to retrieve dependencies.
- `scope`: a comma separated list of scope to include in the artifact filtering (note that the default will rely on `provided` but you can replace it by `compile`, `runtime`, `runtime+compile`, `runtime+system`, `test`).
- `include`: a comma separated list of artifact to include in the artifact filtering.
- `exclude`: a comma separated list of artifact to exclude in the artifact filtering.
- `userArtifacts`: a list of artifacts (groupId, artifactId, version, type - optional, file - optional for plugin transformer, scope - optional) which can be forced inline - mainly useful forÂ `PluginTransformer`.
- `includeTransitiveDependencies`: should transitive dependencies of the components be included, true by default.
- `includeProjectComponentDependencies`: should project component dependencies be included, false by default (normally a job project uses isolation for components so this is not needed).
- `userArtifacts`: set of component artifacts to include.

IMPORTANT: to use with the component tooling, it is recommended to keep default locations. Also if you feel you need to use project dependencies,
you can need to refactor your project structure to ensure you keep component isolation. Talend component let you handle that part but the recommended
practise is to use `userArtifacts` for the components and not the project `<dependencies>`.

===== ContainerDependenciesTransformer

`ContainerDependenciesTransformer` specific configuration is the following one:

- `repositoryBase`: base repository location (default to `MAVEN-INF/repository`).
- `ignoredPaths`: a comma separated list of folder to not create in the output jar, this is common for the ones already created by other transformers/build parts.

===== PluginTransformer

`ContainerDependenciesTransformer` specific configuration is the following one:

- `pluginListResource`: base repository location (default to TALEND-INF/plugins.properties`).

Example: if you want to list only the plugins you use you can configure this transformer like that:

[source,xml,indent=0,subs="verbatim,quotes,attributes"]
----
<transformer implementation="org.talend.sdk.component.container.maven.shade.PluginTransformer">
  <session>${session}</session>
  <include>org.talend.sdk.component:component-x,org.talend.sdk.component:component-y,org.talend.sdk.component:component-z</include>
</transformer>
----

== Configuring component

Component are configured through their constructor parameters. They can all be marked with `@Option`
which will let you give a name to parameters (if not it will use the bytecode name which can require you to compile with `-parameter` flag
to not have `arg0`, `arg1`, ... as names).

The parameter types can be _primitives_ or complex objects with fields decorated with `@Option` exactly like method parameters.

IMPORTANT: it is recommanded to use simple models which can be serialized by components to avoid headaches when implementing serialized components.

Here is an example:

[source,java]
----
class FileFormat implements Serializable {
    @Option("type")
    private FileType type = FileType.CSV;

    @Option("max-records")
    private int maxRecords = 1024;
}

@PartitionMapper(family = "demo", name = "file-reader")
public MyFileReader(@Option("file-path") final File file,
                    @Option("file-format") final FileFormat format) {
    // ...
}
----

Using this kind of API makes the configuration extensible and component oriented letting the user define all he needs.

The instantiation of the parameters is done from the properties passed to the component (see next part).

=== Primitives

What is considered as a primitive in this mecanism is a class which can be directly converted from a `String` to the expected type.

It obviously includes all java primitives, `String` type itself but also all the types with a `org.apache.xbean.propertyeditor.Converter`.

This includes out of the box:

- `BigDecimal`
- `BigInteger`
- `File`
- `InetAddress`
- `ObjectName`
- `URI`
- `URL`
- `Pattern`

=== Complex object mapping

The conversion from properties to object is using the dotted notation. For instance:

[source,properties]
----
file.path = /home/user/input.csv
file.format = CSV
----

will match

[source,java]
----
public class FileOptions {
    @Option("path")
    private File path;

    @Option("format")
    private Format format;
}
----

assuming the method parameter was configured with `@Option("file")`.

==== List case

Lists use the same syntax but to define their elements their rely on an indexed syntax. Assuming the list parameter is named `files`
and the elements are of Â `FileOptions` type, here is how to define a list of 2 elements:

[source,properties]
----
files[0].path = /home/user/input1.csv
files[0].format = CSV
files[1].path = /home/user/input2.xml
files[2].format = EXCEL
----

==== Map case

Inspired from the list case, the map uses `.key[index]` and `.value[index]` to represent its key and values:

[source,properties]
----
// Map<String, FileOptions>
files.key[0] = first-file
files.value[0].path = /home/user/input1.csv
files.value[0].type = CSV
files.key[1] = second-file
files.value[1].path = /home/user/input2.xml
files.value[1].type = EXCEL
----

[source,properties]
----
// Map<FileOptions, String>
files.key[0].path = /home/user/input1.csv
files.key[0].type = CSV
files.value[0] = first-file
files.key[1].path = /home/user/input2.xml
files.key[1].type = EXCEL
files.value[1] = second-file
----

IMPORTANT: don't abuse of map type. If not needed for your configuration (= if you can configure your component
with an object) don't use it.

=== Constraints and validation on the configuration/input

It is common to need to add as metadata a field is required, another has a minimum size etc. This is done with the
validation in `org.talend.sdk.component.api.configuration.constraint` package:

include::{generated_adoc}/constraints.adoc[]

IMPORTANT: using the programmatic API the metadata are prefixed by `tcomp::` but this prefix is stripped in the web for convenience,
the previous table uses the web keys.

=== Marking a configuration as a particular type of data

It is common to classify the incoming data. You can see it as tagging them in several types. The most common ones
are the:

- datastore: all the data you need to connect to the backend
- dataset: a datastore coupled with all the data you need to execute an action

include::{generated_adoc}/configuration-types.adoc[]

IMPORTANT: the component family associated with a configuration type (datastore/dataset) is always the one related
to the component using that configuration.

Those configuration types can be composed to provide one configuration item. For example a dataset type will often need a datastore
type to be provided. and a datastore type (that provides the connection information) will be used to create a dataset type.

Those configuration types will also be used at design time to create shared configuration that can be stored and used at runtime.

For example, we can think about a relational database that support JDBC:

- A datastore may provide:
* jdbc url, username, password
- A dataset may be:
* datastore (that will provide the connection data to the database)
* table name, data []

The component server will scan all those configuration types and provide a configuration type index. This index can be used for the integration
into the targeted platforms (studio, web applications...)

The configuration type index is represented as a flat tree that contains all the configuration types represented as nodes and indexed by their ids.

Also, every node can point to other nodes. This relation is represented as an array of edges that provide the childes ids.

For example, a configuration type index for the above example will be:

[source,json]
----
{nodes: {
             "idForDstore": { datastore:"datastore data", edges:[id:"idForDset"] },
             "idForDset":   { dataset:"dataset data" }
    }
}
----

=== Define links between properties

It can be needed to define a binding between properties, a set of annotations allows to do it:

include::{generated_adoc}/conditions.adoc[]

Target element location is specified as a relative path to current location using Unix path characters.
Configuration class delimiter is `/`. Parent configuration class is specified by `..`.
Thus `../targetProperty` denotes a property, which is located in parent configuration class and has name `targetProperty`.

IMPORTANT: using the programmatic API the metadata are prefixed by `tcomp::` but this prefix is stripped in the web for convenience,
the previous table uses the web keys.

[[documentation-ui-hints]]
=== Add hints about the rendering based on configuration/component knowledge

In some case it can be needed to add some metadata about the configuration to let the UI render properly the configuration.
A simple example is a password value must be hidden and not a simple clear input box. For these cases - when the component developper
wants to influence the UI rendering - you can use a particular set of annotations:

include::{generated_adoc}/ui.adoc[]

IMPORTANT: using the programmatic API the metadata are prefixed by `tcomp::` but this prefix is stripped in the web for convenience,
the previous table uses the web keys.

NOTE: target support should cover `org.talend.core.model.process.EParameterFieldType` but we need to ensure web renderers is able to handle the same widgets.

== Providing some actions for consumers/clients

In some cases you will desire to add some actions unrelated to the runtime. A simple example
is to enable clients - the users of the plugin/library - to test if a connection works. Even more concretely: _does my database is up?_.

To do so you need to define an `@Action` which is a method with a name (representing the event name) in a class decorated with `@Service`:

[source,java]
----
@Service
public class MyDbTester {
    @Action(family = "mycomp", "test")
    public Status doTest(final IncomingData data) {
        return ...;
    }
}
----

IMPORTANT: services are singleton so if you need some thread safety ensure they match that requirement. They
shouldn't store any state too (state is held by the component) since they can be serialized any time.

TIP: services are usable in components as well (matched by type) and allow to reuse some shared logic like a client. Here is a sample
with a service used to access files:

[source,java]
----
@Emitter(family = "sample", name = "reader")
public class PersonReader implements Serializable {
    // attributes skipped to be concise

    public PersonReader(@Option("file") final File file,
                        final FileService service) {
        this.file = file;
        this.service = service;
    }

    // use the service
    @PostConstruct
    public void open() throws FileNotFoundException {
        reader = service.createInput(file);
    }

}
----

TIP: service is passed to constructor automatically, it can be used as a bean. Only call of service's method is required.

=== Particular action types

Some actions are that common and need a clear contract so they are defined as API first citizen, this is the case for wizards or healthchecks
for instance. Here is the list of all actions:

include::{generated_adoc}/actions.adoc[]

== Built in services

The framework provides some built-in services you can inject by type in components and actions out of the box.

Here is the list:

[options="header,autowidth"]
|===
| Type | Description
a| `org.talend.sdk.component.api.service.cache.LocalCache` | Provides a small abstraction to cache data which don't need to be recomputed very often. Commonly used by actions for the UI interactions.
a| `org.talend.sdk.component.api.service.dependency.Resolver` a| Allows to resolve a dependency from its Maven coordinates.
a| `javax.json.spi.JsonProvider` a| A JSON-P instance. Prefer other JSON-P instances if you don't exactly know why you use this one.
a| `javax.json.JsonBuilderFactory` a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations.
a| `javax.json.JsonWriterFactory` a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations.
a| `javax.json.JsonReaderFactory` a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations.
a| `javax.json.stream.JsonParserFactory` a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations.
a| `javax.json.stream.JsonGeneratorFactory` a| A JSON-P instance. It is recommanded to use this one instead of a custom one for memory/speed optimizations.

IMPORTANT: it assumes the dependency is locally available to the execution instance which is not guaranteed yet by the framework.

a| `org.talend.sdk.component.api.service.configuration.LocalConfiguration` a| Represents the local configuration which can be used during the design.

WARNING: it is not recommanded to use it for the runtime since the local configuration is generally different and the instances are distincts.

TIP: you can also use the local cache as an interceptor with `@Cached`
a| Every interface that extends `HttpClient` and that contains methods annotated with `@Request` a| This let you define an http client in a declarative manner using an annotated interface.

TIP: See the <<_httpclient_usage>> for details.

|===

=== HttpClient usage

Let assume that we have a REST API defined like below, and that it requires a basic authentication header.

|===
| GET     `/api/records/{id}` | -
| POST    `/api/records`      | with a json playload to be created `{"id":"some id", "data":"some data"}`
|===

To create an http client able to consume this REST API, we will define an interface that extends `HttpClient`,

The `HttpClient` interface lets you set the `base` for the http address that our client will hit.

The `base` is the part of the address that we will need to add to the request path to hit the api.

Every method annotated with `@Request` of our interface will define an http request.
Also every request can have `@Codec` that let us encode/decode the request/response playloads.

TIP: if your payload(s) is(are) `String` or `Void` you can ignore the coder/decoder.

[source,java]
----
public interface APIClient extends HttpClient {
    @Request(path = "api/records/{id}", method = "GET")
    @Codec(decoder = RecordDecoder.class) //decoder =  decode returned data to Record class
    Record getRecord(@Header("Authorization") String basicAuth, @Path("id") int id);

    @Request(path = "api/records", method = "POST")
    @Codec(encoder = RecordEncoder.class, decoder = RecordDecoder.class) //encoder = encode record to fit request format (json in this example)
    Record createRecord(@Header("Authorization") String basicAuth, Record record);
}
----

IMPORTANT: The interface should extends `HttpClient`.

In the codec classes (class that implement Encoder/Decoder) you can inject any of your services annotated with `@Service` or `@Internationalized` into the constructor.
The i18n services can be useful to have i18n messages for errors handling for example.

This interface can be injected into our Components classes or Services to consume the defined api.
[source,java]
----
@Service
public class MyService {

    private APIClient client;

    public MyService(...,APIClient client){
        //...
        this.client = client;
        client.base("http://localhost:8080");// init the base of the api, ofen in a PostConstruct or init method
    }

    //...
    // Our get request
    Record rec =  client.getRecord("Basic MLFKG?VKFJ", 100);

    //...
    // Our post request
    Record newRecord = client.createRecord("Basic MLFKG?VKFJ", new Record());
}
----

Note: by default `*/*+json` are mapped to JSON-P and `*/*+xml` to JAX-B if the model has a `@XmlRootElement` annotation.

==== Advanced HTTP client request customization

For advanced cases you can customize the `Connection` directly using `@UseConfigurer` on the method.
It will call your custom instance of `Configurer`. Note that you can use some `@ConfigurerOption` in the method
signature to pass some configurer configuration.

For instance if you have this configurer:

[source,java]
----
public class BasicConfigurer implements Configurer {
    @Override
    public void configure(final Connection connection, final ConfigurerConfiguration configuration) {
        final String user = configuration.get("username", String.class);
        final String pwd = configuration.get("password", String.class);
        connection.withHeader(
            "Authorization",
            Base64.getEncoder().encodeToString((user + ':' + pwd).getBytes(StandardCharsets.UTF_8)));
    }
}
----

You can then set it on a method to automatically add the basic header with this kind of API usage:

[source,java]
----
public interface APIClient extends HttpClient {
    @Request(path = "...")
    @UseConfigurer(BasicConfigurer.class)
    Record findRecord(@ConfigurerOption("username") String user, @ConfigurerOption("password") String pwd);
}
----

== Services and interceptors

For common concerns like caching, auditing etc, it can be fancy to use interceptor like API. It is enabled by the framework
on services.

An interceptor defines an annotation marked with `@Intercepts` which defines the implementation of the interceptor (an `InterceptorHandler`).

Here is an example:

[source,java]
----
@Intercepts(LoggingHandler.class)
@Target({ TYPE, METHOD })
@Retention(RUNTIME)
public @interface Logged {
    String value();
}
----

Then handler is created from its constructor and can take service injections (by type). The first parameter, however, can be
a `BiFunction<Method, Object[], Object>` which representes the invocation chain if your interceptor can be used with others.

IMPORTANT: if you do a generic interceptor it is important to pass the invoker as first parameter. If you don't do so
you can't combine interceptors at all.

Here is an interceptor implementation for our `@Logged` API:

[source,java]
----
public class LoggingHandler implements InterceptorHandler {
    // injected
    private final BiFunction<Method, Object[], Object> invoker;
    private final SomeService service;

    // internal
    private final ConcurrentMap<Method, String> loggerNames = new ConcurrentHashMap<>();

    public CacheHandler(final BiFunction<Method, Object[], Object> invoker, final SomeService service) {
        this.invoker = invoker;
        this.service = service;
    }

    @Override
    public Object invoke(final Method method, final Object[] args) {
        final String name = loggerNames.computeIfAbsent(method, m -> findAnnotation(m, Logged.class).get().value());
        service.getLogger(name).info("Invoking {}", method.getName());
        return invoker.apply(method, args);
    }
}
----

This implementation is compatible with interceptor chains since it takes the invoker as first constructor parameter
and it also takes a service injection. Then the implementation just does what is needed - logging the invoked method here.

NOTE: the `findAnnotation` annotation - inherited from `InterceptorHandler` is an utility method to find an annotation on a method
or class (in this order).

== Creating an execution chain or pipeline

There are two kind of chains the framework supports:

1. standalone chains
2. link:https://beam.apache.org/[Beam] chains

=== Standalone chains

For now the standalone chains only support linear flows (more to come) and are built based on a `ExecutionChainBuilder`.

This one will take some metadata about the flow (which input with which configuration, which processors, which processor order and configuration...)
and will execute it linearly:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
ExecutionChainBuilder.start()
        .withConfiguration("SampleJob", true) <1>
        .withInput("sample", "reader", 1, new HashMap<String, String>() {{ <2>
            put("file", "/tmp/input.csv");
        }})
        .toProcessor(null "sample", "mapper", 2, emptyMap()) <2>
        .toProcessor("reject", "sample", "writer", new HashMap<String, String>() {{ <2>
            put("file", "/tmp/output.csv");
        }}).getParent()
        .toProcessor("reject", "sample", "writer", 1, new HashMap<String, String>() {{ <2>
            put("file", "/tmp/output.csv");
        }})
        .create(manager, plugin -> null, new CountingSuccessListener(), new ToleratingErrorHandler(0)) <3>
        .get()
        .execute(); <4>
----

You can configure some global execution configuration, like giving it a name and if system properties can override the component configuration
using `<component group name>.<component name>.<configuration property name>`.

Each component is registered from its component name (what is passed in `family` of the component annotation),
its own name (`name` method of the annotation) and its configuration as a map (2).

Once the flow is defined (there is no more processor after), then you can create the `ExecutionChain` (3) with its success/error listeners
to handle it in a custom manner and finally you can get it and execute the flow/pipeline (4).

A chain needs a `ComponentManager` which is the manager of the plugins. The easiest is to use its default constructor to create one instance.

IMPORTANT: this manager needs to be closed when no more needed so don't forget to call close.

Here is a sample code for the previous chain:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
public class Main {
    public static void main(final String[] args) {
        final ComponentManager manager = ComponentManager.instance()
        ExecutionChainBuilder.start()
                .withConfiguration("SampleJob", true)
                .fromInput("sample", "reader", 1, new HashMap<String, String>() {{
                    put("file", "/tmp/input.csv");
                }})
                .toProcessor("rejected", "sample", "mapper", 1, emptyMap())
                .getParent()
                .toProcessor(Branches.DEFAULT_BRANCH, "sample", "mapper", 1, emptyMap())
                .toProcessor(Branches.DEFAULT_BRANCH, "sample", "writer", 3, new HashMap<String, String>() {{
                    put("file", "/tmp/output.csv");
                }})
                .create(manager, plugin -> null, new CountingSuccessListener(), new ToleratingErrorHandler(0))
                .get()
                .execute();
    }
}
----

NOTE: this API is not very powerful but allows to test simple cases, you can have a look to beam direct runner and the Talend Component beam bridge
for more advanced cases.

TIP: the number before the configuration (map) is the version of this configuration, it allows to automatically migrate
from a version to the currently available one when needed.

=== Beam case

For beam case, you need to rely on beam pipeline definition and use `component-runtime-beam` dependency which provides Beam bridges.

==== I/O

`org.talend.sdk.component.runtime.beam.TalendIO` provides a way to convert a partition mapper or a processor to an input or processor
using the `read` or `write` methods.

[source,java]
----
public class Main {
    public static void main(final String[] args) {
        final ComponentManager manager = ComponentManager.instance()
        Pipeline pipeline = Pipeline.create();
        //Create beam input from mapper and apply input to pipeline
        pipeline.apply(TalendIO.read(manager.findMapper(manager.findMapper("sample", "reader", 1, new HashMap<String, String>() {{
                    put("fileprefix", "input");
                }}).get()))
                .apply(new ViewsMappingTransform<>(emptyMap())) // prepare it for the output record format (see next part)
        //Create beam processor from talend processor and apply to pipeline
                .apply(TalendIO.write(manager.findProcessor("test", "writer", 1, new HashMap<String, String>() {{
                    put("fileprefix", "output");
                }}).get(), emptyMap()));

        //... run pipeline
    }
}
----

==== Processors

`org.talend.sdk.component.runtime.beam.TalendFn` provides the way to wrap a processor in a Beam `PTransform` and integrate it in the pipeline.

[source,java]
----
public class Main {
    public static void main(final String[] args) {
        //Component manager and pipeline initialization...

        //Create beam PTransform from processor and apply input to pipeline
        pipeline.apply(TalendFn.asFn(manager.findProcessor("sample", "mapper", 1, emptyMap())).get())), emptyMap());

        //... run pipeline
    }
}
----

The multiple inputs/outputs are represented by a `Map` element in beam case to avoid to use multiple inputs/outputs.

TIP: you can use `ViewsMappingTransform` or `CoGroupByKeyResultMappingTransform` to adapt the input/output
format to this `Map<String, List<Serializable>>`.

==== Deployment

IMPORTANT: Beam serializing components it is crucial to add `component-runtime-standalone` dependency to the project. It will take
care of providing an implicit and lazy `ComponentManager` managing the component in a fatjar case.

==== Convert a Beam.io in a component I/O

For simple I/O you can get automatic conversion of the Beam.io to a component I/O transparently if you decorated your `PTransform`
with `@PartitionMapper` or `@Processor`.

The limitation are:

- Inputs must implement `PTransform<PBegin, PCollection<?>>` and must be a `BoundedSource`.
- Outputs must implement `PTransform<PCollection<?>, PDone>` and just register on the input `PCollection` a `DoFn`.

More information on that topic on <<wrapping-a-beam-io.adoc#, How to wrap a Beam I/O>> page.

== Maven Plugin

`talend-component-maven-plugin` intends to help you to write components
validating components match best practices and also generating transparently metadata used by Talend Studio.

Here is how to use it:

[source,xml]
----
<plugin>
  <groupId>org.talend.sdk.component</groupId>
  <artifactId>talend-component-maven-plugin</artifactId>
  <version>${component.version}</version>
</plugin>
----

Note that this plugin is also an extension so you can declare it in your `build/extensions` block as:

[source,xml]
----
<extension>
  <groupId>org.talend.sdk.component</groupId>
  <artifactId>talend-component-maven-plugin</artifactId>
  <version>${component.version}</version>
</extension>
----

Used as an extension, `dependencies`, `validate` and `documentation` goals will be set up.

=== Dependencies

The first goal is a shortcut for the `maven-dependency-plugin`, it will create the `TALEND-INF/dependencies.txt` file
with the `compile` and `runtime` dependencies to let the component use it at runtime:

[source,xml]
----
<plugin>
  <groupId>org.talend.sdk.component</groupId>
  <artifactId>talend-component-maven-plugin</artifactId>
  <version>${component.version}</version>
  <executions>
    <execution>
      <id>talend-dependencies</id>
      <goals>
        <goal>dependencies</goal>
      </goals>
    </execution>
  </executions>
</plugin>
----

=== Validate

The most important goal is here to help you to validate the common programming model of the component. Here is the execution definition to activate it:

[source,xml]
----
<plugin>
  <groupId>org.talend.sdk.component</groupId>
  <artifactId>talend-component-maven-plugin</artifactId>
  <version>${component.version}</version>
  <executions>
    <execution>
      <id>talend-component-validate</id>
      <goals>
        <goal>validate</goal>
      </goals>
    </execution>
  </executions>
</plugin>
----

By default it will be bound to `process-classes` phase. When executing it will do several validations which can be switched off
adding the corresponding flags to `false` in the `<configuration>` block of the execution:

[options="header,autowidth"]
|===
|Name|Description|Default
|validateInternationalization|Validates resource bundle are presents and contain commonly used keys (like `_displayName`)|true
|validateModel|Ensure components pass validations of the `ComponentManager` and Talend Component runtime|true
|validateSerializable|Ensure components are `Serializable` - note this is a sanity check, the component is not actually serialized here, if you have a doubt ensure to test it. It also checks any `@Internationalized` class is valid and has its keys.|true
|validateMetadata|Ensure components define an `@Icon` and `@Version`.|true
|validateDataStore|Ensure any  `@DataStore` defines a `@HealthCheck`.|true
|validateComponent|Ensure native programming model is respected, you can disable it when using another programming model like in beam case.|true
|validateActions|Validate actions signatures for the ones not tolerating dynamic binding (`@HealthCheck`, `@DynamicValues`, ...). It is recommanded to keep it `true`.|true
|validateFamily|Validate the family, i.e. the package containing the `@Components` has also a `@Icon`.|true
|validateDocumentation|Ensure all 1. components and 2. `@Option` properties have a documentation using `@Documentation`|true
|===

=== Documentation

This goal generates an Asciidoc file documenting your component from the configuration model (`@Option`) and
`@Documentation` you can put on options and the component itself.

[source,xml]
----
<plugin>
  <groupId>org.talend.sdk.component</groupId>
  <artifactId>talend-component-maven-plugin</artifactId>
  <version>${component.version}</version>
  <executions>
    <execution>
      <id>talend-component-documentation</id>
      <goals>
        <goal>asciidoc</goal>
      </goals>
    </execution>
  </executions>
</plugin>
----

[options="header,autowidth"]
|===
|Name|Description|Default
|level
|Which level are the root title
a|2 which means `==`

|output
a|Where to store the output, it is *NOT* recommended to change it
a|`${classes}/TALEND-INF/documentation.adoc`

|formats
|A map of the renderings to do, keys are the format (`pdf` or `html`) and values the output paths
| -

|attributes
|A map of asciidoctor attributes when formats is set
| -

|templateDir / templateEngine
|Template configuration for the rendering
| -

|title
|Document title
| ${project.name}

|attachDocumentations
|Should the documentations (`.adoc`, and `formats` keys) should be attached to the project (and deployed)
| true
|===

TIP: if you use the extension you can add the property `talend.documentation.htmlAndPdf` and set it to `true` in your project
to automatically get a html and PDF rendering of the documentation.

==== Render your documentation

===== HTML

To render the generated documentation you can use the Asciidoctor Maven plugin (or Gradle equivalent):

[source,xml]
----
<plugin> (1)
  <groupId>org.talend.sdk.component</groupId>
  <artifactId>talend-component-maven-plugin</artifactId>
  <version>${talend-component-kit.version}</version>
  <executions>
    <execution>
      <id>documentation</id>
      <phase>prepare-package</phase>
      <goals>
        <goal>asciidoc</goal>
      </goals>
    </execution>
  </executions>
</plugin>
<plugin> (2)
  <groupId>org.asciidoctor</groupId>
  <artifactId>asciidoctor-maven-plugin</artifactId>
  <version>1.5.6</version>
  <executions>
    <execution>
      <id>doc-html</id>
      <phase>prepare-package</phase>
      <goals>
        <goal>process-asciidoc</goal>
      </goals>
      <configuration>
        <sourceDirectory>${project.build.outputDirectory}/TALEND-INF</sourceDirectory>
        <sourceDocumentName>documentation.adoc</sourceDocumentName>
        <outputDirectory>${project.build.directory}/documentation</outputDirectory>
        <backend>html5</backend>
      </configuration>
    </execution>
  </executions>
</plugin>
----

1. Will generate in `target/classes/TALEND-INF/documentation.adoc` the components documentation.
2. Will render the documenation as an html file in `target/documentation/documentation.html`.

TIP: ensure to execute it after the documentation generation.

===== PDF

If you prefer a PDF rendering you can configure the following execution
in the asciidoctor plugin (note that you can configure both executions if you want
both HTML and PDF rendering):

[source,xml]
----
<plugin>
  <groupId>org.asciidoctor</groupId>
  <artifactId>asciidoctor-maven-plugin</artifactId>
  <version>1.5.6</version>
  <executions>
    <execution>
      <id>doc-html</id>
      <phase>prepare-package</phase>
      <goals>
        <goal>process-asciidoc</goal>
      </goals>
      <configuration>
        <sourceDirectory>${project.build.outputDirectory}/TALEND-INF</sourceDirectory>
        <sourceDocumentName>documentation.adoc</sourceDocumentName>
        <outputDirectory>${project.build.directory}/documentation</outputDirectory>
        <backend>pdf</backend>
      </configuration>
    </execution>
  </executions>
  <dependencies>
    <dependency>
      <groupId>org.asciidoctor</groupId>
      <artifactId>asciidoctorj-pdf</artifactId>
      <version>1.5.0-alpha.16</version>
    </dependency>
  </dependencies>
</plugin>
----

===== Include the documentation into a document

If you want to add some more content or add a title, you can include the generated document into
another document using Asciidoc `include` directive.

A common example is:

[source,adoc]
----
= Super Components
Super Writer
:toc:
:toclevels: 3
:source-highlighter: prettify
:numbered:
:icons: font
:hide-uri-scheme:
:imagesdir: images

include::{generated_doc}/documentation.adoc[]
----

This assumes you pass to the plugin the attribute `generated_doc`, this can be done this way:

[source,xml]
----
<plugin>
  <groupId>org.asciidoctor</groupId>
  <artifactId>asciidoctor-maven-plugin</artifactId>
  <version>1.5.6</version>
  <executions>
    <execution>
      <id>doc-html</id>
      <phase>prepare-package</phase>
      <goals>
        <goal>process-asciidoc</goal>
      </goals>
      <configuration>
        <sourceDirectory>${project.basedir}/src/main/asciidoc</sourceDirectory>
        <sourceDocumentName>my-main-doc.adoc</sourceDocumentName>
        <outputDirectory>${project.build.directory}/documentation</outputDirectory>
        <backend>html5</backend>
        <attributes>
          <generated_adoc>${project.build.outputDirectory}/TALEND-INF</generated_adoc>
        </attributes>
      </configuration>
    </execution>
  </executions>
</plugin>
----

This is optional but allows to reuse maven placeholders to pass paths which is quite convenient in an automated build.

===== More

You can find more customizations on Asciidoctor link:http://asciidoctor.org/docs/asciidoctor-maven-plugin/[website].

=== Web

Testing the rendering of your component(s) configuration into the Studio is just a matter of deploying a component
in Talend Studio (you can have a look to link::studio.html[Studio Documentation] page. But don't forget
the component can also be deployed into a Cloud (web) environment. To ease the testing of the related rendering,
you can use the goal `web` of the plugin:

[source]
----
mvn talend-component:web
----

Then you can test your component going on http://localhost:8080. You need to select which component form you want
to see using the treeview on the left, then on the right the form will be displayed.

The two available configurations of the plugin are `serverPort` which is a shortcut to change the default, 8080, port
of the embedded server and `serverArguments` to pass Meecrowave options to the server. More on that configuration
is available at http://openwebbeans.apache.org/meecrowave/meecrowave-core/cli.html.

=== Generate inputs or outputs

The Mojo `generate` (maven plugin goal) of the same plugin also embeds a generator you can use to bootstrap any input or output component:

[source,xml]
----
<plugin>
  <groupId>org.talend.sdk.component</groupId>
  <artifactId>talend-component-maven-plugin</artifactId>
  <version>${talend-component.version}</version>
  <executions>
    <execution> <1>
      <id>generate-input</id>
      <phase>generate-sources</phase>
      <goals>
        <goal>generate</goal>
      </goals>
      <configuration>
        <type>input</type>
      </configuration>
    </execution>
    <execution> <2>
      <id>generate-output</id>
      <phase>generate-sources</phase>
      <goals>
        <goal>generate</goal>
      </goals>
      <configuration>
        <type>output</type>
      </configuration>
    </execution>
  </executions>
</plugin>
----

<1> Generates an input (partition mapper + emitter)
<2> Generates an output

It is intended to be used from the command line (or IDE Maven integration):

[source,sh]
----
$ mvn talend-component:generate \
    -Dtalend.generator.type=[input|output] \ <1>
    [-Dtalend.generator.classbase=com.test.MyComponent] \ <2>
    [-Dtalend.generator.family=my-family] \ <3>
    [-Dtalend.generator.pom.read-only=false] <4>
----

<1> select the type of component you want, `input` to generate a mapper and emitter and `output` to generate an output processor
<2> set the class name base (will be suffixed by the component type), if not set the package will be guessed and classname based on the basedir name
<3> set the component family to use, default to the base dir name removing (component[s] from the name, ex: `my-component` will lead to `my` as family if not explicitly set)
<4> should the generator try to add `component-api` in the pom if not already here, if you added it you can set it to `false` directly in the pom

For this command to work you will need to just register the plugin:

[source,xml]
----
<plugin>
  <groupId>org.talend.sdk.component</groupId>
  <artifactId>talend-component-maven-plugin</artifactId>
  <version>${talend-component.version}</version>
</plugin>
----

== Gradle Plugin

`gradle-talend-component` intends to help you to write components
validating components match best practices. It is inspired from the Maven plugin
and adds the ability to generate automatically the `dependencies.txt` file the SDK
uses to build the component classpath. For more information on the configuration
you can check out the maven properties matching the attributes.

Here is how to use it:

[source,groovy]
----
buildscript {
  repositories {
    mavenLocal()
    mavenCentral()
  }
  dependencies {
    classpath "org.talend.sdk.component:gradle-talend-component:${talendComponentVersion}"
  }
}

apply plugin: 'org.talend.sdk.component'
apply plugin: 'java'

// optional customization
talendComponentKit {
    // dependencies.txt generation, replaces maven-dependency-plugin
    dependenciesLocation = "TALEND-INF/dependencies.txt"
    boolean skipDependenciesFile = false;

    // classpath for validation utilities
    sdkVersion = "${talendComponentVersion}"
    apiVersion = "${talendComponentApiVersion}"

    // documentation
    skipDocumentation = false
    documentationOutput = new File(....)
    documentationLevel = 2 // first level will be == in the generated adoc
    documentationTitle = 'My Component Family' // default to project name
    documentationFormats = [:] // adoc attributes
    documentationFormats = [:] // renderings to do

    // validation
    skipValidation = false
    validateFamily = true
    validateSerializable = true
    validateInternationalization = true
    validateModel = true
    validateMetadata = true
    validateComponent = true
    validateDataStore = true
    validateDataSet = true
    validateActions = true

    // web
    serverArguments = []
    serverPort = 8080
}
----

== Advanced: define a custom API

It is possible to extend the Component API for custom front features.

What is important here is to keep in mind you should do it
only if it targets not portable components (only used by the Studio or Beam).

In term of organization it is recommanded to create a custom `xxxx-component-api` module with the new set of annotations.

=== Extending the UI

To extend the UI just add an annotation which can be put on `@Option` fields which is decorated with `@Ui`.
All its members will be put in the metadata of the parameter. Example:

[source,java]
----
@Ui
@Target(TYPE)
@Retention(RUNTIME)
public @interface MyLayout {
}
----
