:toc:
:numbered:
:icons: font
:hide-uri-scheme:
:imagesdir: images
:outdir: ../assets
:jbake-type: page
:jbake-tags: documentation
:jbake-status: published

== Creating an execution chain or pipeline

There are two kind of chains the framework supports:

1. standalone chains
2. link:https://beam.apache.org/[Beam] chains

=== Standalone chains

For now the standalone chains only support linear flows (more to come) and are built based on a `ExecutionChainBuilder`.

This one will take some metadata about the flow (which input with which configuration, which processors, which processor order and configuration...)
and will execute it linearly:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
ExecutionChainBuilder.start()
        .withConfiguration("SampleJob", true) <1>
        .withInput("sample", "reader", 1, new HashMap<String, String>() {{ <2>
            put("file", "/tmp/input.csv");
        }})
        .toProcessor(null "sample", "mapper", 2, emptyMap()) <2>
        .toProcessor("reject", "sample", "writer", new HashMap<String, String>() {{ <2>
            put("file", "/tmp/output.csv");
        }}).getParent()
        .toProcessor("reject", "sample", "writer", 1, new HashMap<String, String>() {{ <2>
            put("file", "/tmp/output.csv");
        }})
        .create(manager, plugin -> null, new CountingSuccessListener(), new ToleratingErrorHandler(0)) <3>
        .get()
        .execute(); <4>
----

You can configure some global execution configuration, like giving it a name and if system properties can override the component configuration
using `<component group name>.<component name>.<configuration property name>`.

Each component is registered from its component name (what is passed in `family` of the component annotation),
its own name (`name` method of the annotation) and its configuration as a map (2).

Once the flow is defined (there is no more processor after), then you can create the `ExecutionChain` (3) with its success/error listeners
to handle it in a custom manner and finally you can get it and execute the flow/pipeline (4).

A chain needs a `ComponentManager` which is the manager of the plugins. The easiest is to use its default constructor to create one instance.

IMPORTANT: this manager needs to be closed when no more needed so don't forget to call close.

Here is a sample code for the previous chain:

[source,java,indent=0,subs="verbatim,quotes,attributes"]
----
public class Main {
    public static void main(final String[] args) {
        final ComponentManager manager = ComponentManager.instance()
        ExecutionChainBuilder.start()
                .withConfiguration("SampleJob", true)
                .fromInput("sample", "reader", 1, new HashMap<String, String>() {{
                    put("file", "/tmp/input.csv");
                }})
                .toProcessor("rejected", "sample", "mapper", 1, emptyMap())
                .getParent()
                .toProcessor(Branches.DEFAULT_BRANCH, "sample", "mapper", 1, emptyMap())
                .toProcessor(Branches.DEFAULT_BRANCH, "sample", "writer", 3, new HashMap<String, String>() {{
                    put("file", "/tmp/output.csv");
                }})
                .create(manager, plugin -> null, new CountingSuccessListener(), new ToleratingErrorHandler(0))
                .get()
                .execute();
    }
}
----

NOTE: this API is not very powerful but allows to test simple cases, you can have a look to beam direct runner and the Talend Component beam bridge
for more advanced cases.

TIP: the number before the configuration (map) is the version of this configuration, it allows to automatically migrate
from a version to the currently available one when needed.

==== String based DSL

`ExecutionChainDsl` exposes a way to define pipeline based on URI instead of previous syntax:

[source,java]
----
from("chain://list?__version=1&values[0]=a&values[1]=bb&values[2]=ccc")
    .to("chain://count?__version=1")
    .to("chain://file?__version=1&file=" + output.getAbsolutePath())
    .create()
    .execute();
----

=== Beam case

For beam case, you need to rely on beam pipeline definition and use `component-runtime-beam` dependency which provides Beam bridges.

==== I/O

`org.talend.sdk.component.runtime.beam.TalendIO` provides a way to convert a partition mapper or a processor to an input or processor
using the `read` or `write` methods.

[source,java]
----
public class Main {
    public static void main(final String[] args) {
        final ComponentManager manager = ComponentManager.instance()
        Pipeline pipeline = Pipeline.create();
        //Create beam input from mapper and apply input to pipeline
        pipeline.apply(TalendIO.read(manager.findMapper(manager.findMapper("sample", "reader", 1, new HashMap<String, String>() {{
                    put("fileprefix", "input");
                }}).get()))
                .apply(new ViewsMappingTransform(emptyMap(), "sample")) // prepare it for the output record format (see next part)
        //Create beam processor from talend processor and apply to pipeline
                .apply(TalendIO.write(manager.findProcessor("test", "writer", 1, new HashMap<String, String>() {{
                    put("fileprefix", "output");
                }}).get(), emptyMap()));

        //... run pipeline
    }
}
----

==== Processors

`org.talend.sdk.component.runtime.beam.TalendFn` provides the way to wrap a processor in a Beam `PTransform` and integrate it in the pipeline.

[source,java]
----
public class Main {
    public static void main(final String[] args) {
        //Component manager and pipeline initialization...

        //Create beam PTransform from processor and apply input to pipeline
        pipeline.apply(TalendFn.asFn(manager.findProcessor("sample", "mapper", 1, emptyMap())).get())), emptyMap());

        //... run pipeline
    }
}
----

The multiple inputs/outputs are represented by a `Map` element in beam case to avoid to use multiple inputs/outputs.

TIP: you can use `ViewsMappingTransform` or `CoGroupByKeyResultMappingTransform` to adapt the input/output
format to the record format representing the multiple inputs/output, so a kind of `Map<String, List<?>>`,
but materialized as a `JsonObject`. Input data must be of type `JsonObject` in this case.

==== Deployment

IMPORTANT: Beam serializing components it is crucial to add `component-runtime-standalone` dependency to the project. It will take
care of providing an implicit and lazy `ComponentManager` managing the component in a fatjar case.

==== Convert a Beam.io in a component I/O

For simple I/O you can get automatic conversion of the Beam.io to a component I/O transparently if you decorated your `PTransform`
with `@PartitionMapper` or `@Processor`.

The limitation are:

- Inputs must implement `PTransform<PBegin, PCollection<?>>` and must be a `BoundedSource`.
- Outputs must implement `PTransform<PCollection<?>, PDone>` and just register on the input `PCollection` a `DoFn`.

More information on that topic on <<wrapping-a-beam-io.adoc#, How to wrap a Beam I/O>> page.